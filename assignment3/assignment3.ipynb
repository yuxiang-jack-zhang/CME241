{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2026) - Assignment 3\n",
    "\n",
    "**Due: Friday, February 27 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Jeffrey Xue\n",
    "- Jack Zhang\n",
    "- Patrick Flanagan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize_scalar\n",
    "from typing import Iterable, Iterator, Mapping, TypeVar, Callable, Sequence, Tuple, Dict\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "from rl.chapter10.prediction_utils import compare_td_and_mc\n",
    "X = TypeVar('X')\n",
    "S = TypeVar('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Milk Vendor Optimization Problem (Led by Jack Zhang)\n",
    "\n",
    "You are a milk vendor and your task is to bring to your store a supply (denoted $S \\in \\mathbb{R}$) of milk volume in the morning that will give you the best profits. You know that the demand for milk throughout the course of the day is a probability distribution function $f$ (for mathematical convenience, assume people can buy milk in volumes that are real numbers, hence milk demand $x \\in \\mathbb{R}$ is a continuous variable with a probability density function). \n",
    "\n",
    "For every extra gallon of milk you carry at the end of the day (supply $S$ exceeds random demand $x$), you incur a cost of $h$ (effectively the wasteful purchases amounting to the difference between your purchase price and the end-of-day discount disposal price since you are not allowed to sell the same milk the next day). For every gallon of milk that a customer demands that you don’t carry (random demand $x$ exceeds supply $S$), you incur a cost of $p$ (effectively the missed sales revenue amounting to the difference between your sales price and purchase price). \n",
    "\n",
    "Your task is to identify the optimal supply $S$ that minimizes your **Expected Cost** $g(S)$, given by:\n",
    "\n",
    "$$\n",
    "g_1(S) = \\mathbb{E}[\\max(x - S, 0)] = \\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_2(S) = \\mathbb{E}[\\max(S - x, 0)] = \\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(S) = p \\cdot g_1(S) + h \\cdot g_2(S)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Analytical Optimization\n",
    "\n",
    "1. **Derive the first-order condition (FOC)** for minimizing the expected cost $g(S)$.\n",
    "2. **Solve the FOC** to express the optimal supply $S^*$ in terms of the given parameters: $p$, $h$, and the demand distribution $f(x)$. (*Hint*: Pay attention to the balance between the costs of overstocking and understocking)\n",
    "\n",
    "3. **Interpretation**: Provide an interpretation of the condition you derived. What does the balance between $p$ and $h$ imply about the optimal supply $S^*$?\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Special Case Analysis\n",
    "\n",
    "1. Consider the case where the demand $x$ follows an **exponential distribution** with parameter $\\lambda > 0$. That is, $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$.\n",
    "    - Derive an explicit expression for the optimal supply $S^*$.\n",
    "    \n",
    "2. Consider the case where the demand $x$ follows a **normal distribution** with mean $\\mu$ and variance $\\sigma^2$, i.e., $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$. \n",
    "    - Set up the integral for $g(S)$ and describe how it relates to the **cumulative distribution function (CDF)** of the normal distribution.\n",
    "    - Provide an interpretation of how changes in $\\mu$ and $\\sigma$ influence the optimal $S^*$. \n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Framing as a Financial Options Problem\n",
    "\n",
    "1. Frame the milk vendor’s problem as a **portfolio of call and put options**:\n",
    "    - Identify the analog of the “strike price” and “underlying asset.”\n",
    "    - Explain which part of the cost function $g_1(S)$ or $g_2(S)$ corresponds to a call option and which part to a put option.\n",
    "    - What do $p$ and $h$ represent in this options framework?\n",
    "\n",
    "2. Explain how this framing could be used to derive the optimal supply $S^*$ if solved using financial engineering concepts.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Numerical Solution and Simulation\n",
    "\n",
    "1. **Numerical Solution**: Write a Python function that numerically estimates the optimal $S^*$ using an iterative search or numerical optimization method. \n",
    "\n",
    "2. **Simulation**: Generate random samples of milk demand from an exponential distribution and simulate the total costs for different values of $S$. Plot the costs against $S$ and visually identify the optimal $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "**1.**\n",
    "\n",
    "Writing out the expected cost:\n",
    "\n",
    "$$g(S) = p \\int_S^\\infty (x - S) f(x)\\,dx + h \\int_{-\\infty}^S (S - x) f(x)\\,dx$$\n",
    "\n",
    "Lets define $F$ to be the CDF of the demand distribution.\n",
    "\n",
    "Differentiating with respect to $S$ to find FOC:\n",
    "\n",
    "$$g_1'(S) = \\frac{d}{dS}\\int_S^\\infty (x-S)f(x)\\,dx = -(S-S)f(S) + \\int_S^\\infty (-1)f(x)\\,dx = -(1 - F(S))$$\n",
    "\n",
    "$$g_2'(S) = (S-S)f(S) + \\int_{-\\infty}^S (1)f(x)\\,dx = F(S)$$\n",
    "\n",
    "Putting those results together and set to 0:\n",
    "\n",
    "$$g'(S) = -p\\,(1 - F(S)) + h\\,F(S)=0$$\n",
    "\n",
    "$$-p + pF(S) + hF(S) = 0$$\n",
    "\n",
    "$$F(S)(p + h) = p$$\n",
    "\n",
    "$$F(S) = \\frac{p}{p + h},$$\n",
    "\n",
    "which is the FOC.\n",
    "\n",
    "**2.**\n",
    "\n",
    "Solving the FOC for $S^*$ is simple:\n",
    "\n",
    "$$S^* = F^{-1}\\!\\left(\\frac{p}{p+h}\\right),$$\n",
    "\n",
    "where $F$ is still the CDF of the demand distribution.\n",
    "\n",
    "**3. Interpretation**\n",
    "\n",
    "The optimal supply is a ratio between $p$ the understock cost and $h$ the overstock cost. Specifically, it's the percentage of understock to total cost (understock + overstock). \n",
    "\n",
    "When understock is much more expensive than overstock, meaning $p \\gg h$, the ratio goes $1$. This means the vendor should stock as much milk as possible since the cost of understock dominates the cost of overstock. On the other hand, when overstock is much more expensive, $h \\gg p$, $S^*$ goes to $0$, meaning the vendor stocks very little to avoid waste.\n",
    "\n",
    "When $p = h$, $S^*=0.5$, meaning the optimal supply is the median of the demand distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "**1.:** $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$, with CDF $F(x) = 1 - e^{-\\lambda x}$.\n",
    "\n",
    "Applying the FOC $F(S^*) = \\frac{p}{p+h}$:\n",
    "\n",
    "$$1 - e^{-\\lambda S^*} = \\frac{p}{p+h}$$\n",
    "\n",
    "$$e^{-\\lambda S^*} = \\frac{h}{p+h}$$\n",
    "\n",
    "$$S^* = \\frac{1}{\\lambda}\\ln\\!\\left(\\frac{p+h}{h}\\right)$$\n",
    "\n",
    "**2.:** $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with CDF $F(x) = \\Phi\\!\\left(\\frac{x - \\mu}{\\sigma}\\right)$.\n",
    "\n",
    "$$g(S) = p \\int_S^\\infty (x - S)\\,\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)dx \\;+\\; h \\int_{-\\infty}^S (S - x)\\,\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)dx$$\n",
    "\n",
    "Let's define some variables to make the expression nicer. Let $z = \\frac{S - \\mu}{\\sigma}$ and $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}$.\n",
    "\n",
    "$$g_1(S) = (\\mu - S)(1 - \\Phi(z)) + \\sigma\\,\\phi(z)$$\n",
    "\n",
    "$$g_2(S) = (S - \\mu)\\,\\Phi(z) + \\sigma\\,\\phi(z)$$\n",
    "\n",
    "Apply the FOC $F(S^*) = \\frac{p}{p+h}$.\n",
    "\n",
    "$$\\Phi\\!\\left(\\frac{S^* - \\mu}{\\sigma}\\right) = \\frac{p}{p+h}$$\n",
    "\n",
    "$$S^* = \\mu + \\sigma\\,\\Phi^{-1}\\!\\left(\\frac{p}{p+h}\\right)$$\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Mean $\\mu$: Supply scales linearly with mean. If $\\mu$ goes up $S^*$ goes up by the same amount. Higher expected demand requires just as much more supply.\n",
    "\n",
    "Variance $\\sigma^2$: Depends on the sign of $\\Phi^{-1}\\!\\left(\\frac{p}{p+h}\\right)$. \n",
    "\n",
    "If $p > h$ (understocking is more expensive), then $\\frac{p}{p+h} > 0.5$, so $\\Phi^{-1} > 0$, and increasing $\\sigma$ raises $S^*$. This means more uncertainty warrants a larger safety stock since we want to avoid understocking.\n",
    "\n",
    "On the other hand, if $p < h$ (overstocking is more expensive), then $\\frac{p}{p+h} < 0.5$, so $\\Phi^{-1} < 0$, and increasing $\\sigma$ **lowers** $S^*$ This means more uncertainty warrants a smaller safety stock since we want to avoid overstockign.\n",
    "\n",
    "If $p = h$, then $\\Phi^{-1}(0.5) = 0$ and $S^* = \\mu$ independent of $\\sigma$. We don't need to consider uncertainty because understocking and overstocking risks are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "**1.**\n",
    "\n",
    "The random demand $x$ is the underlying asset price at expiry, and the supply level $S$ plays is the strike price.\n",
    "\n",
    "The understocking cost $g_1(S) = \\mathbb{E}[\\max(x - S, 0)]$ has the same form as the expected payoff of a call option with strike $S$ on underlying $x$. The overstocking cost $g_2(S) = \\mathbb{E}[\\max(S - x, 0)]$ has the same form of the expected payoff of a put option.\n",
    "\n",
    "The per-unit costs $p$ and $h$ are the number of contracts on the call and put. The total expected cost is then the value of a portfolio of $p$ calls and $h$ puts, all with strike $S$:\n",
    "\n",
    "$$g(S) = p \\cdot \\text{Call}(S) + h \\cdot \\text{Put}(S)$$\n",
    "\n",
    "**2.**\n",
    "\n",
    "We can use put-call parity $\\text{Call}(S) - \\text{Put}(S) = \\mathbb{E}[x] - S$\n",
    "\n",
    "to write\n",
    "\n",
    "$$g(S) = p\\,\\text{Call}(S) + h\\,\\text{Put}(S) = p\\,\\text{Call}(S) + h\\,(\\text{Call}(S) - \\mathbb{E}[x] + S)$$\n",
    "$$= (p+h)\\,\\text{Call}(S) + h(S - \\mathbb{E}[x])$$\n",
    "\n",
    "We can take the derivative and use the option delta $\\frac{d}{dS}\\text{Call}(S) = -(1-F(S))$.\n",
    "\n",
    "$$g'(S) = -(p+h)(1-F(S)) + h = 0 $$\n",
    "$$F(S^*) = \\frac{p}{p+h}$$\n",
    "\n",
    "In other words, the optimal strike is chosen so that the probability of the option finishing ITM (in the money) equals the ratio of contracts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical S* = 1.2528\n"
     ]
    }
   ],
   "source": [
    "from scipy import integrate\n",
    "import numpy as np\n",
    "\n",
    "# simulation parameters\n",
    "p = 5  # understock cost \n",
    "h = 2  # overstock cost \n",
    "lambda_param = 1.0  # for exponential distribution\n",
    "\n",
    "def demand_pdf(x):\n",
    "    return lambda_param * np.exp(-lambda_param * x) if x >= 0 else 0.0\n",
    "\n",
    "def demand_cdf(x):\n",
    "    return 1 - np.exp(-lambda_param * x) if x >= 0 else 0.0\n",
    "\n",
    "def expected_cost(S):\n",
    "    # g1(S): expected understock cost\n",
    "    g1, _ = integrate.quad(lambda x: (x - S) * demand_pdf(x), S, np.inf)\n",
    "    # g2(S): expected overstock cost\n",
    "    g2, _ = integrate.quad(lambda x: (S - x) * demand_pdf(x), 0, S)\n",
    "    return p * g1 + h * g2\n",
    "\n",
    "# analytical solution\n",
    "analytical_S_star = (1 / lambda_param) * np.log((p + h) / h)\n",
    "print(f\"Analytical S* = {analytical_S_star:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.252763051168189)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = minimize_scalar(expected_cost, bounds=(0, 10), method='bounded')\n",
    "optimal_S = result.x\n",
    "\n",
    "# Simulation of costs\n",
    "S_values = np.linspace(0, 10, 500)\n",
    "costs = [expected_cost(S) for S in S_values]\n",
    "\n",
    "# Plotting the costs against S\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(S_values, costs, label=\"Expected Cost $g(S)$\", color='blue')\n",
    "plt.axvline(optimal_S, color='red', linestyle='--', label=f\"Optimal $S^* \\\\approx {optimal_S:.2f}$\")\n",
    "plt.xlabel(\"Supply $S$\")\n",
    "plt.ylabel(\"Expected Cost $g(S)$\")\n",
    "plt.title(\"Expected Cost $g(S)$ vs. Supply $S$ (Exponential Demand)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "optimal_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Car Sales (Led By: Jeffrey Xue)\n",
    "\n",
    "You must sell your car within a finite window of $N$ days. At the beginning of each day $t \\in \\{1, \\dots, N\\}$, you receive a single offer $X_t$ from a dealership, where $\\{X_t\\}$ are i.i.d. draws from a known continuous distribution $Q$ supported on $[m, M]$, with $0 < m < M$.\n",
    "\n",
    "After observing $X_t$, you must immediately choose whether to **accept** or **reject** the offer:\n",
    "\n",
    "- If you **accept** on day $t$, you sell the car immediately and receive payoff $X_t$. The process then terminates.\n",
    "- If you **reject** on day $t < N$, you keep the car, pay a parking cost $c \\ge 0$ at the end of that day, and proceed to day $t+1$.\n",
    "- On day $N$, you must accept the offer (i.e., rejecting is not allowed).\n",
    "\n",
    "Your objective is to maximize the expected **net payoff**, defined as the sale price minus the total parking costs paid before the sale.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "Model this problem as a finite-horizon **Markov Decision Process (MDP)**. Clearly specify:\n",
    "\n",
    "1. **States**\n",
    "2. **Actions**\n",
    "3. **Rewards**\n",
    "4. **State-transition probabilities**\n",
    "5. **Discount factor** (use $\\gamma = 1$)\n",
    "\n",
    "Also state whether this MDP is finite- or infinite-horizon, episodic or continuing, and whether it is time-homogeneous.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Structure of the Optimal Policy\n",
    "\n",
    "Let $V_t$ denote the optimal value function when there are $t$ days remaining (before observing the offer).\n",
    "\n",
    "1. Write down the Bellman recursion for $V_t$.\n",
    "2. Show that the optimal policy is a **time-dependent threshold policy**: that is, there exists a reservation price $r_t$ such that it is optimal to accept an offer $x$ if and only if $x \\ge r_t$.\n",
    "3. Express the threshold $r_t$ in terms of $V_{t-1}$ and $c$.\n",
    "\n",
    "(You may leave expectations with respect to $Q$ written in integral or expectation form. A closed-form solution for general $Q$ is not required.)\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Special Case (No Integrals)\n",
    "\n",
    "Now consider the special case where $c = 0$ and $Q = \\text{Uniform}[m, M]$.\n",
    "\n",
    "1. Write the recursion for $V_t$ explicitly.\n",
    "2. Provide the optimal policy in as closed-form a way as possible. Your policy may depend on recursively defined coefficients, but your final expressions should contain **no integrals**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "1. **States**\n",
    "Each decision point is $(t, x)$ where $t \\in \\{1, \\dots, N\\}$ is the current day and $x \\in [m, M]$ is the offer received that day. Any state where $t = N$ or the car is sold results in termination which we can represent as the absorbing state $\\textsf{Sold}$.\n",
    "\n",
    "2. **Actions**\n",
    "For $t < N$: the action space is $\\{\\text{Accept}, \\text{Reject}\\}$.\n",
    "For $t = N$: the action space is just $\\{\\text{Accept}\\}$ (i.e. must accept).\n",
    "\n",
    "3. **Rewards**\n",
    "If the action chosen is $\\text{Accept}$ at state $(t, x)$, the reward $= x$.\n",
    "If the action chosen is $\\text{Reject}$ at state $(t, x)$ (in which case it must be true that $t < N$), the reward $= -c$.\n",
    "\n",
    "4. **State-transition probabilities**\n",
    "Any state that choses the action $\\text{Accept}$ will transition to the terminal state $\\textsf{Sold}$ deterministically. \n",
    "Otherwise, state $(t, x)$ transitions to $(t+1, X_{t+1})$ where $X_{t+1} \\sim Q$ independently.\n",
    "\n",
    "5. **Discount factor** (use $\\gamma = 1$)\n",
    "\n",
    "The MDP is also finite-horizon with at most $N$ steps, episodic since every trajectory ends when the car is sold, and time-inhomogeneous because the action set and the number of remaining days depend on $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "1. Bellman base case ($t = 1$): we have to accept, so the value is just the expectation of the offer. Bellman recursive case ($t \\geq 2$): we should reject the offer if the value of waiting for tomorrow's offer (including parking cost) is greater. Thus, we take the max of $X$ and $V_{t-1} - c$, so the value today is the expectation of this max.\n",
    "$$V_1 = \\mathbb{E}_Q[X]$$\n",
    "$$V_t = \\mathbb{E}_Q\\!\\Big[\\max\\!\\big(X,\\; V_{t-1} - c\\big)\\Big]$$\n",
    "\n",
    "2. Following the logic above, given the offer $x$ on a day with $t$ remaining days, the agent accepts iff $x \\;\\geq\\; V_{t-1} - c$. Thus, the reservation price is $r_t = V_{t-1} - c$. Both $V_{t-1}$ and $c$ are independent of $x$, so this is a valid reservation price to accept an offer $x$ if and only if $x \\ge r_t.$ Thus, this a time-dependent threshold policy.\n",
    "\n",
    "3. As explained above, $r_t = V_{t-1} - c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "1. As outlined in part (B), the base case is $V_1 = \\mathbb{E}[X] = \\frac{m + M}{2}$ and the recursive case is \n",
    "$$V_t = \\mathbb{E}_Q\\!\\Big[\\max\\!\\big(X,\\; V_{t-1} - c\\big)\\Big] = \\mathbb{E}_Q\\!\\Big[\\max\\!\\big(X,\\; V_{t-1}\\big)\\Big] = V_{t-1}\\cdot\\frac{V_{t-1}-m}{M-m} + \\int_{V_{t-1}}^{M} x \\cdot \\frac{1}{M-m}\\,dx.$$\n",
    "$$= V_{t-1}\\cdot\\frac{V_{t-1}-m}{M-m} \\;+\\; \\frac{M^2 - V_{t-1}^2}{2(M-m)}  = \\frac{V_{t-1}^2 - 2m\\,V_{t-1} + M^2}{2(M-m)}.$$\n",
    "\n",
    "2. We can express the optimal policy in terms of the reservation price, as the policy is just to accept an offer $x$ if and only if $x \\ge r_t.$ From our answer in part (B), we have $r_1 = m$ because we must accept on the last day, and $r_t = V_{t-1} - c = V_{t-1}$ for  $t \\geq 2.$ Therefore, once we solve for  $\\{V_1, V_2, \\ldots, V_N\\}$, we can get all our $r_t$ values which give us the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Constrained Consumption (Led By: Jeffrey Xue)\n",
    "\n",
    "Consider the following discrete-time MDP for constrained consumption. At $t = 0$, the agent is given a finite amount $x_0 \\in \\mathbb{R}^+$ of a resource. In each time period, the agent can choose to consume any amount of the resource, with the consumption denoted as $c \\in [0, x]$ where $x$ is the amount of the resource remaining at the start of the time period. This consumption results in a reduction of the resource at the start of the next time period:  \n",
    "$$x' = x - c.$$  \n",
    "\n",
    "Consuming a quantity $c$ of the resource provides a utility of consumption equal to $U(c)$, and we adopt the **CRRA utility function**:  \n",
    "$$\n",
    "U(c) = \\frac{c^{1 - \\gamma}}{1 - \\gamma}, \\quad (\\gamma > 0, \\gamma \\neq 1)\n",
    "$$\n",
    "\n",
    "Our goal is to maximize the aggregate discounted utility of consumption until the resource is completely consumed. We assume a discount factor of $\\beta \\in (0, 1)$ when discounting the utility of consumption over any single time period. Assume parameters are such that the value function is finite.\n",
    "\n",
    "We model this as a **discrete-time, continuous-state-space, continuous-action-space, stationary, deterministic MDP**, and so our goal is to solve for the **Optimal Value Function** and associated **Optimal Policy**, which will give us the optimal consumption trajectory of the resource. Since this is a stationary MDP, the **State** is simply the amount $x$ of the resource remaining at the start of a time period. The **Action** is the consumption quantity $c$ in that time period. The **Reward** for a time period is $U(c)$ when the consumption in that time period is $c$. The discount factor over each single time period is $\\beta$.\n",
    "\n",
    "We assume that the **Optimal Policy** is given by:  \n",
    "$$\n",
    "c^* = \\theta^* \\cdot x \\quad \\text{for some } \\theta^* \\in [0, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Closed-form Expression for $V_\\theta(x)$\n",
    "\n",
    "Our first step is to consider a fixed deterministic policy, given by:  \n",
    "$$c = \\theta \\cdot x \\quad \\text{for some fixed } \\theta \\in [0, 1].$$  \n",
    "Derive a closed-form expression for the **Value Function** $V_\\theta(x)$ for a fixed deterministic policy, given by $c = \\theta \\cdot x$. Specifically, you need to express $V_\\theta(x)$ in terms of $\\beta$, $\\gamma$, $\\theta$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Solving for $\\theta^*$\n",
    "\n",
    "Use this closed-form expression for $V_\\theta(x)$ to solve for the $\\theta^*$ which maximizes $V_\\theta(x)$ (thus fetching us the **Optimal Policy** given by $c^* = \\theta^* \\cdot x$).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Expression for $V^*(x)$\n",
    "\n",
    "Use this expression for $\\theta^*$ to obtain an expression for the **Optimal Value Function** $V^*(x)$ in terms of only $\\beta$, $\\gamma$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Validation of the Bellman Equation\n",
    "\n",
    "Validate that the **Optimal Policy** (derived in part B) and **Optimal Value Function** (derived in part C) satisfy the **Bellman Optimality Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "With this policy, we have $x' = x - c = x - \\theta x = (1 - \\theta) x,$ so the state evolves deterministically with $x_t = (1-\\theta)^t x.$\n",
    "\n",
    "The consumption will be $c_t = \\theta \\cdot x_t = \\theta(1-\\theta)^t x$, which gets utility\n",
    "\n",
    "$$U(c_t) = \\frac{(\\theta(1-\\theta)^t x)^{1-\\gamma}}{1-\\gamma} = \\frac{\\theta^{1-\\gamma}(1-\\theta)^{t(1-\\gamma)} x^{1-\\gamma}}{1-\\gamma}.$$\n",
    "\n",
    "Summing the discounted utilities gets\n",
    "\n",
    "$$V_\\theta(x) = \\sum_{t=0}^{\\infty} \\beta^t U(c_t) = \\frac{\\theta^{1-\\gamma}\\, x^{1-\\gamma}}{1-\\gamma} \\sum_{t=0}^{\\infty} \\big(\\beta(1-\\theta)^{1-\\gamma}\\big)^t.$$\n",
    "$$= \\frac{\\theta^{1-\\gamma}x^{1-\\gamma}}{1-\\gamma} \\cdot \\frac{1}{1 - \\beta(1-\\theta)^{1-\\gamma}}.$$\n",
    "assuming geometric series convergence with $\\beta(1-\\theta)^{1-\\gamma} < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "Since $V_\\theta(x) = \\frac{x^{1-\\gamma}}{1-\\gamma} \\cdot \\frac{\\theta^{1-\\gamma}}{1 - \\beta(1-\\theta)^{1-\\gamma}}$ and $\\frac{x^{1-\\gamma}}{1-\\gamma}$ is a constant in $\\theta$, we maximize $V_\\theta(x)$ by optimizing over $g(\\theta) = \\frac{\\theta^{1-\\gamma}}{1 - \\beta(1-\\theta)^{1-\\gamma}}$.\n",
    "\n",
    "This gets:\n",
    "$$g'(\\theta) = 0$$\n",
    "$$\\frac{(1-\\gamma)\\theta^{-\\gamma}\\big[1 - \\beta(1-\\theta)^{1-\\gamma}\\big] - \\theta^{1-\\gamma}\\cdot\\beta(1-\\gamma)(1-\\theta)^{-\\gamma}}{(1 - \\beta(1-\\theta)^{1-\\gamma})^2} = 0$$\n",
    "$$(1-\\gamma)\\theta^{-\\gamma}\\big[1 - \\beta(1-\\theta)^{1-\\gamma}\\big] - \\theta^{1-\\gamma}\\cdot\\beta(1-\\gamma)(1-\\theta)^{-\\gamma} = 0$$\n",
    "$$\\theta^{-\\gamma}\\big[1 - \\beta(1-\\theta)^{1-\\gamma}\\big] = \\beta\\,\\theta^{1-\\gamma}(1-\\theta)^{-\\gamma}$$\n",
    "$$\\theta^{-\\gamma} = \\theta^{-\\gamma}\\beta(1-\\theta)^{1-\\gamma} + \\beta\\,\\theta^{1-\\gamma}(1-\\theta)^{-\\gamma}$$\n",
    "$$\\theta^{-\\gamma} = \\beta\\Big[\\theta^{-\\gamma}(1-\\theta)^{1-\\gamma} + \\theta^{1-\\gamma}(1-\\theta)^{-\\gamma}\\Big]$$\n",
    "$$\\theta^{-\\gamma} = \\beta\\big[\\theta(1-\\theta)\\big]^{-\\gamma}\\big[(1-\\theta)+\\theta\\big]$$\n",
    "$$\\theta^{-\\gamma} = \\beta\\big[\\theta(1-\\theta)\\big]^{-\\gamma}$$\n",
    "$$1 = \\beta(1-\\theta)^{-\\gamma}$$\n",
    "$$(1-\\theta)^\\gamma = \\beta$$\n",
    "Solving gets the optimal policy $\\theta^* = 1 - \\beta^{1/\\gamma}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "Substituting $\\theta^* = 1 - \\beta^{1/\\gamma}$ into the answer in part (A):\n",
    "$$V^*(x) = V_{\\theta^*}(x)$$\n",
    "$$= \\frac{(\\theta^*)^{1-\\gamma}x^{1-\\gamma}}{1-\\gamma } \\cdot \\frac{1}{1 - \\beta(1-\\theta^*)^{1-\\gamma}}$$\n",
    "$$ = \\frac{(1 - \\beta^{1/\\gamma})^{1-\\gamma}x^{1-\\gamma}}{1-\\gamma } \\cdot \\frac{1}{1 - \\beta(1-(1 - \\beta^{1/\\gamma}))^{1-\\gamma}}$$\n",
    "$$ = \\frac{(1 - \\beta^{1/\\gamma})^{1-\\gamma}x^{1-\\gamma}}{1-\\gamma } \\cdot \\frac{1}{1 - \\beta^{1/\\gamma}}$$\n",
    "$$ = \\frac{(1 - \\beta^{1/\\gamma})^{-\\gamma}}{1-\\gamma}\\, x^{1-\\gamma}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "First, let $K = (1 - \\beta^{1/\\gamma})^{-\\gamma}$ so that $V^*(x) = \\frac{K\\, x^{1-\\gamma}}{1-\\gamma}$. We want to show $V^*(x)$ and $c^* = \\theta^* x$ satisfy the Bellman Optimality Equation:\n",
    "$$V^*(x) = \\max\\{U(c) + \\beta\\, V^*(x - c)\\} = \\max\\{\\frac{c^{1-\\gamma}}{1-\\gamma} + \\frac{\\beta K\\,(x-c)^{1-\\gamma}}{1-\\gamma}\\}$$\n",
    "\n",
    "Taking a derivative and setting to 0 gets \n",
    "$$c^{-\\gamma} - \\beta K\\,(x-c)^{-\\gamma} = 0$$\n",
    "$$c^{-\\gamma} = \\beta K\\,(x-c)^{-\\gamma}$$\n",
    "$$\\left(\\frac{x-c}{c}\\right)^\\gamma = \\beta K$$\n",
    "$$\\frac{x-c}{c} = (\\beta K)^{1/\\gamma}$$\n",
    "$$\\frac{x}{c} - 1 = (\\beta (1 - \\beta^{1/\\gamma})^{-\\gamma})^{1/\\gamma}$$\n",
    "$$\\frac{x}{c} - 1 = \\frac{\\beta^{1/\\gamma}}{1 - \\beta^{1/\\gamma}}$$\n",
    "$$\\frac{x}{c} = 1 + \\frac{\\beta^{1/\\gamma}}{1-\\beta^{1/\\gamma}} $$\n",
    "$$\\frac{x}{c} = \\frac{1}{1-\\beta^{1/\\gamma}}$$\n",
    "$$\\frac{x}{c} = \\frac{1}{\\theta^*} $$\n",
    "$$c = \\theta^* x$$\n",
    "\n",
    "This confirms the optimal policy from part (B).\n",
    "\n",
    "Now, going back to $V^*(x) = \\max\\{\\frac{c^{1-\\gamma}}{1-\\gamma} + \\frac{\\beta K\\,(x-c)^{1-\\gamma}}{1-\\gamma}\\}$, substituting $c^* = \\theta^* x$ into the objective gets:\n",
    "$$\\frac{(\\theta^* x)^{1-\\gamma}}{1-\\gamma} + \\frac{\\beta K\\,((1-\\theta^*)x)^{1-\\gamma}}{1-\\gamma} = \\frac{x^{1-\\gamma}}{1-\\gamma}\\Big((\\theta^*)^{1-\\gamma} + \\beta K\\,(1-\\theta^*)^{1-\\gamma}\\Big).$$\n",
    "\n",
    "Since $1-\\theta^* = \\beta^{1/\\gamma}$ and $K = (\\theta^*)^{-\\gamma}$, we get\n",
    "$$\\frac{x^{1-\\gamma}}{1-\\gamma}\\Big((\\theta^*)^{1-\\gamma} + \\beta K\\,(1-\\theta^*)^{1-\\gamma}\\Big)$$\n",
    "$$= \\frac{x^{1-\\gamma}}{1-\\gamma}\\Big((\\theta^*)^{1-\\gamma} + \\beta\\,(\\theta^*)^{-\\gamma}\\,\\beta^{(1-\\gamma)/\\gamma}\\Big)$$\n",
    "$$= \\frac{x^{1-\\gamma}}{1-\\gamma}\\Big((\\theta^*)^{1-\\gamma} + \\beta^{1/\\gamma}(\\theta^*)^{-\\gamma}\\Big)$$\n",
    "$$= \\frac{x^{1-\\gamma}}{1-\\gamma}\\Big((\\theta^*)^{-\\gamma}\\big(\\theta^* + \\beta^{1/\\gamma}\\big)\\Big)$$\n",
    "$$= \\frac{x^{1-\\gamma}}{1-\\gamma}\\Big((\\theta^*)^{-\\gamma}\\big((1-\\beta^{1/\\gamma}) + \\beta^{1/\\gamma}\\big)\\Big)$$\n",
    "$$= \\frac{x^{1-\\gamma}}{1-\\gamma}(\\theta^*)^{-\\gamma}$$\n",
    "$$= \\frac{x^{1-\\gamma}}{1-\\gamma}((1 - \\beta^{1/\\gamma}))^{-\\gamma}$$\n",
    "$$= V^*(x)$$\n",
    "\n",
    "This matches the lefthand side of the Bellman Optimality Equation, so it confirms the optimal value function from part (C)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Double Q-Learning (Led By: Patrick Flanagan)\n",
    "\n",
    "It is known that **Q-Learning** can suffer from maximization bias during finite-sample training. In this problem, we consider a modification of tabular Q-Learning called **Double Q-Learning**, which reduces this bias by decoupling action selection and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 1: Double Q-Learning**\n",
    "\n",
    "**Initialize** $Q_1(s,a)$ and $Q_2(s,a)$ for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$  \n",
    "**yield** estimate of $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, set $t = 0$  \n",
    "&emsp; **while** $s_t$ is non-terminal **do**  \n",
    "\n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy with respect to  \n",
    "&emsp;&emsp; $\\displaystyle \\pi(s) = \\arg\\max_a \\big( Q_1(s,a) + Q_2(s,a) \\big)$  \n",
    "\n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "\n",
    "&emsp;&emsp; **with probability 0.5 update $Q_1$:**\n",
    "\n",
    "&emsp;&emsp;&emsp; Let  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle a^* = \\arg\\max_a Q_1(s_{t+1}, a)$  \n",
    "\n",
    "&emsp;&emsp;&emsp; Update  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle \n",
    "Q_1(s_t, a_t) \\leftarrow Q_1(s_t, a_t)\n",
    "+ \\alpha \\Big(\n",
    "r_t + \\gamma Q_2(s_{t+1}, a^*)\n",
    "- Q_1(s_t, a_t)\n",
    "\\Big)\n",
    "$\n",
    "\n",
    "&emsp;&emsp; **otherwise update $Q_2$:**\n",
    "\n",
    "&emsp;&emsp;&emsp; Let  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle a^* = \\arg\\max_a Q_2(s_{t+1}, a)$  \n",
    "\n",
    "&emsp;&emsp;&emsp; Update  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle \n",
    "Q_2(s_t, a_t) \\leftarrow Q_2(s_t, a_t)\n",
    "+ \\alpha \\Big(\n",
    "r_t + \\gamma Q_1(s_{t+1}, a^*)\n",
    "- Q_2(s_t, a_t)\n",
    "\\Big)\n",
    "$\n",
    "\n",
    "&emsp;&emsp; $t \\leftarrow t + 1$  \n",
    "&emsp;&emsp; $s_t \\leftarrow s_{t+1}$  \n",
    "\n",
    "**yield** estimate $Q_1 + Q_2$\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 2: Q-Learning**\n",
    "\n",
    "**Initialize** $Q(s,a)$ for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$  \n",
    "**yield** $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, set $t = 0$  \n",
    "&emsp; **while** $s_t$ is non-terminal **do**  \n",
    "\n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy with respect to  \n",
    "&emsp;&emsp; $\\displaystyle \\pi(s) = \\arg\\max_a Q(s,a)$  \n",
    "\n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "\n",
    "&emsp;&emsp; Update  \n",
    "&emsp;&emsp; $\\displaystyle \n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t)\n",
    "+ \\alpha \\Big(\n",
    "r_t + \\gamma \\max_a Q(s_{t+1}, a)\n",
    "- Q(s_t, a_t)\n",
    "\\Big)\n",
    "$\n",
    "\n",
    "&emsp;&emsp; $t \\leftarrow t + 1$  \n",
    "&emsp;&emsp; $s_t \\leftarrow s_{t+1}$  \n",
    "\n",
    "**yield** $Q$\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "The code skeleton for this problem is provided below. Implement the functions:\n",
    "\n",
    "- `double_q_learning`\n",
    "- `q_learning`\n",
    "\n",
    "After running both algorithms, you will obtain a plot of the estimated Q-value versus episode number.\n",
    "\n",
    "In your writeup:\n",
    "\n",
    "1. Compare the behavior of Q-Learning and Double Q-Learning.\n",
    "2. Explain why Q-Learning can exhibit maximization bias.\n",
    "3. Discuss the advantages and possible drawbacks of Double Q-Learning in general MDPs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable, Generic\n",
    "\n",
    "# RL imports (adapt or remove if you don't have the same environment):\n",
    "from rl.distribution import (\n",
    "    Distribution, Constant, Gaussian, Choose, SampledDistribution, Categorical\n",
    ")\n",
    "from rl.markov_process import NonTerminal, State, Terminal\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.td import epsilon_greedy_action\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Tabular Q-value function approximation (done for you)\n",
    "# -----------------------------------------------------------------------\n",
    "class TabularQValueFunctionApprox(Generic[S, A]):\n",
    "    \"\"\"\n",
    "    A basic implementation of a tabular function approximation \n",
    "    with constant learning rate of 0.1\n",
    "    Also tracks the number of updates per (state, action).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.counts: Mapping[Tuple[NonTerminal[S], A], int] = defaultdict(int)\n",
    "        self.values: Mapping[Tuple[NonTerminal[S], A], float] = defaultdict(float)\n",
    "    \n",
    "    def update(self, k: Tuple[NonTerminal[S], A], target: float) -> None:\n",
    "        alpha = 0.1\n",
    "        old_val = self.values[k]\n",
    "        self.values[k] = (1 - alpha) * old_val + alpha * target\n",
    "        self.counts[k] += 1\n",
    "    \n",
    "    def __call__(self, x: Tuple[NonTerminal[S], A]) -> float:\n",
    "        return self.values[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Double Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def double_q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Implements Double Q-Learning as described:\n",
    "      1) We keep two Q-tables, Q1 and Q2.\n",
    "      2) We choose actions epsilon-greedily with respect to Q1+Q2.\n",
    "      3) With 50% chance we update Q1 using next-action chosen by max of Q1,\n",
    "         evaluated by Q2; otherwise update Q2 symmetrically.\n",
    "    \"\"\"\n",
    "    Q1 = TabularQValueFunctionApprox()\n",
    "    Q2 = TabularQValueFunctionApprox()\n",
    "    epsilon = 0.1\n",
    "\n",
    "    # Wrapper so epsilon_greedy_action selects w.r.t. Q1 + Q2\n",
    "    # Returns the average so the yield is a proper Q-value estimate\n",
    "    # (argmax is scale-invariant, so this works for action selection too)\n",
    "    class CombinedQ:\n",
    "        def __call__(self, x):\n",
    "            return (Q1(x) + Q2(x)) / 2\n",
    "\n",
    "    combined_q = CombinedQ()\n",
    "\n",
    "    while True:\n",
    "        s = states.sample()\n",
    "        while isinstance(s, NonTerminal):\n",
    "            actions = set(mdp.actions(s))\n",
    "            a = epsilon_greedy_action(combined_q, s, actions, epsilon)\n",
    "            s_next, r = mdp.step(s, a).sample()\n",
    "\n",
    "            if isinstance(s_next, NonTerminal):\n",
    "                next_actions = list(mdp.actions(s_next))\n",
    "                if random.random() < 0.5:\n",
    "                    # Update Q1: select a* with Q1, evaluate with Q2\n",
    "                    a_star = max(next_actions, key=lambda a_: Q1((s_next, a_)))\n",
    "                    target = r + gamma * Q2((s_next, a_star))\n",
    "                    Q1.update((s, a), target)\n",
    "                else:\n",
    "                    # Update Q2: select a* with Q2, evaluate with Q1\n",
    "                    a_star = max(next_actions, key=lambda a_: Q2((s_next, a_)))\n",
    "                    target = r + gamma * Q1((s_next, a_star))\n",
    "                    Q2.update((s, a), target)\n",
    "            else:\n",
    "                # Terminal state: target is just r\n",
    "                if random.random() < 0.5:\n",
    "                    Q1.update((s, a), r)\n",
    "                else:\n",
    "                    Q2.update((s, a), r)\n",
    "\n",
    "            s = s_next\n",
    "\n",
    "        yield combined_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Standard Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Standard Q-Learning:\n",
    "      1) Keep one Q table\n",
    "      2) Epsilon-greedy wrt that table\n",
    "      3) Update Q((s,a)) with  r + gamma * max_{a'} Q((s_next, a'))\n",
    "    \"\"\"\n",
    "    Q = TabularQValueFunctionApprox()\n",
    "    epsilon = 0.1\n",
    "\n",
    "    while True:\n",
    "        s = states.sample()\n",
    "        while isinstance(s, NonTerminal):\n",
    "            actions = set(mdp.actions(s))\n",
    "            a = epsilon_greedy_action(Q, s, actions, epsilon)\n",
    "            s_next, r = mdp.step(s, a).sample()\n",
    "\n",
    "            if isinstance(s_next, NonTerminal):\n",
    "                next_actions = list(mdp.actions(s_next))\n",
    "                max_q = max(Q((s_next, a_)) for a_ in next_actions)\n",
    "                target = r + gamma * max_q\n",
    "            else:\n",
    "                # Terminal state: no future value\n",
    "                target = r\n",
    "\n",
    "            Q.update((s, a), target)\n",
    "            s = s_next\n",
    "\n",
    "        yield Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# The MDP: States A,B and actions a1,a2,b1,...,bn (don't modify anything anymore, just run to get the graphs)\n",
    "# -----------------------------------------------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class P1State:\n",
    "    \"\"\"\n",
    "    The MDP state, storing whether we are in \"A\" or \"B\".\n",
    "    \"\"\"\n",
    "    name: str\n",
    "\n",
    "class P1MDP(MarkovDecisionProcess[P1State, str]):\n",
    "    \n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "\n",
    "    def actions(self, state: NonTerminal[P1State]) -> Iterable[str]:\n",
    "        \"\"\"\n",
    "        Return the actions available from this state.\n",
    "          - if state is A => [\"a1\", \"a2\"]\n",
    "          - if state is B => [\"b1\", ..., \"bn\"]\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            return [\"a1\", \"a2\"]\n",
    "        else:\n",
    "            return [f\"b{i}\" for i in range(1, self.n+1)]\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[P1State],\n",
    "        action: str\n",
    "    ) -> Distribution[Tuple[State[P1State], float]]:\n",
    "        \"\"\"\n",
    "        Return the distribution of (next state, reward) from (state, action):\n",
    "          - A + a1 => reward 0, next state B\n",
    "          - A + a2 => reward 0, next state terminal\n",
    "          - B + b_i => reward ~ Normal(-0.1,1), next state terminal\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            if action == \"a1\":\n",
    "                return Constant((NonTerminal(P1State(\"B\")), 0.0))\n",
    "            else:\n",
    "                return Constant((Terminal(P1State(\"T\")), 0.0))\n",
    "        else:\n",
    "            # For B + b_i => reward ~ N(-0.1,1), then terminal\n",
    "            def sampler():\n",
    "                r = np.random.normal(loc=-0.1, scale=1.0)\n",
    "                return (Terminal(P1State(\"T\")), r)\n",
    "            return SampledDistribution(sampler)\n",
    "\n",
    "def run_double_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Runs one 'chain' of Double Q-Learning for 'episodes' episodes,\n",
    "    returning a list of Q-values for Q((A,a1)) at the end of each episode.\n",
    "    \"\"\"\n",
    "    dq_iter = double_q_learning(mdp, start_dist, gamma)  # generator\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q1 = next(dq_iter)\n",
    "        # record Q1((A,a1)) each time\n",
    "        qA1 = Q1((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def run_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Same but for standard Q-Learning\n",
    "    \"\"\"\n",
    "    q_iter = q_learning(mdp, start_dist, gamma)\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q = next(q_iter)\n",
    "        qA1 = Q((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def main():\n",
    "    # For reproducibility\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n = 10\n",
    "    mdp = P1MDP(n)\n",
    "    # Always start in A, as a NonTerminal\n",
    "    start_dist = Constant(NonTerminal(P1State(\"A\")))\n",
    "\n",
    "    N_RUNS = 100\n",
    "    N_EPISODES = 400\n",
    "\n",
    "    all_dbl = []\n",
    "    all_std = []\n",
    "\n",
    "    for _ in range(N_RUNS):\n",
    "        dbl_vals = run_double_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        std_vals = run_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        all_dbl.append(dbl_vals)\n",
    "        all_std.append(std_vals)\n",
    "\n",
    "    arr_dbl = np.array(all_dbl)\n",
    "    arr_std = np.array(all_std)\n",
    "\n",
    "    avg_dbl = np.mean(arr_dbl, axis=0)\n",
    "    avg_std = np.mean(arr_std, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(avg_dbl, label='Double Q-Learning: Q(A,a1)')\n",
    "    plt.plot(avg_std, label='Q-Learning: Q(A,a1)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Estimated Q-value')\n",
    "    plt.title('Average Q(A,a1) over 100 runs, n=10')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writeup\n",
    "\n",
    "#### 1. Comparing the Behavior of Q-Learning and Double Q-Learning\n",
    "\n",
    "In this MDP, state $A$ offers two actions: $a_1$ (transition to $B$ with reward $0$) and $a_2$ (transition to the terminal state with reward $0$). From state $B$, there are $n = 10$ actions $b_1, \\dots, b_{10}$, each producing a reward drawn from $\\mathcal{N}(-0.1, 1)$ and transitioning to the terminal state. The true optimal Q-values are $Q^*(A, a_1) = -0.1$ and $Q^*(A, a_2) = 0$, so the optimal policy from $A$ is to choose $a_2$.\n",
    "\n",
    "The plot reveals a clear divergence in behavior:\n",
    "\n",
    "- **Q-Learning** significantly overestimates $Q(A, a_1)$ in early episodes, often producing positive estimates despite the true value being $-0.1$. This causes the agent to incorrectly prefer $a_1$ over $a_2$, leading to suboptimal behavior. The overestimation persists for many episodes before the estimate slowly decays toward the true value.\n",
    "\n",
    "- **Double Q-Learning** produces estimates of $Q(A, a_1)$ that remain much closer to $-0.1$ throughout training. It avoids the initial spike of overestimation and converges more reliably to the correct value.\n",
    "\n",
    "#### 2. Why Q-Learning Exhibits Maximization Bias\n",
    "\n",
    "The update target in standard Q-Learning is\n",
    "\n",
    "$$r + \\gamma \\max_{a'} Q(s', a').$$\n",
    "\n",
    "In state $B$, Q-Learning computes $\\max_{i} Q(B, b_i)$ over $10$ actions, all of which share the same true Q-value of $-0.1$. Because the rewards are stochastic ($\\sigma = 1$), the Q-estimates $Q(B, b_i)$ are noisy. Taking the maximum of these noisy estimates introduces a systematic upward bias:\n",
    "\n",
    "$$\\mathbb{E}\\!\\Big[\\max_i \\hat{Q}(B, b_i)\\Big] \\;>\\; \\max_i\\, Q^*(B, b_i) = -0.1.$$\n",
    "\n",
    "This follows from Jensen's inequality applied to the convex $\\max$ function. With $n = 10$ actions and high-variance returns, the bias is substantial.\n",
    "\n",
    "The root cause is that Q-Learning uses the **same** Q-table for two coupled purposes: (1) **selecting** the maximizing action via $\\arg\\max_a Q(s', a)$, and (2) **evaluating** that action's value via $Q(s', a^*)$. The action selected by $\\arg\\max$ is, by construction, the one with the highest noisy estimate, so its value is systematically inflated. This inflated estimate then propagates backward: the target for $Q(A, a_1)$ incorporates the biased $\\max$ from $B$, causing overestimation at $A$ as well.\n",
    "\n",
    "In our implementation, this manifests directly in the `q_learning` function, where the target is computed as:\n",
    "```python\n",
    "max_q = max(Q((s_next, a_)) for a_ in next_actions)\n",
    "target = r + gamma * max_q\n",
    "```\n",
    "The same `Q` table both identifies and scores the best next action, coupling selection and evaluation.\n",
    "\n",
    "#### 3. Advantages and Drawbacks of Double Q-Learning\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- **Eliminates maximization bias** by decoupling action selection from value evaluation. When updating $Q_1$, the next action $a^* = \\arg\\max_a Q_1(s', a)$ is selected using $Q_1$, but its value is evaluated using the independent estimator $Q_2(s', a^*)$. Since $Q_2$'s estimation noise is independent of $Q_1$'s $\\arg\\max$, the positive correlation that drives overestimation is broken. This is implemented in our `double_q_learning` function:\n",
    "    ```python\n",
    "    a_star = max(next_actions, key=lambda a_: Q1((s_next, a_)))\n",
    "    target = r + gamma * Q2((s_next, a_star))\n",
    "    ```\n",
    "- **More accurate value estimates** in environments where many actions share similar true values but have high-variance returns, which is exactly the regime where maximization bias is most severe.\n",
    "- **Minimal implementation overhead**: the only additions over standard Q-Learning are a second Q-table, a coin flip (`random.random() < 0.5`) to decide which table to update, and a `CombinedQ` wrapper for action selection. The per-step cost is negligible.\n",
    "\n",
    "**Drawbacks:**\n",
    "\n",
    "- **Slower convergence per table**: each Q-table only receives updates $50\\%$ of the time, so each individual table learns from half the experience. In environments where maximization bias is not a concern (e.g., few actions or low-variance rewards), this slower learning may outweigh the bias-reduction benefit.\n",
    "- **Doubled memory**: two complete Q-tables must be stored. In the tabular setting this is trivial, but when scaling to function approximation (e.g., Deep Double DQN), this requires maintaining two neural networks, increasing both memory and computation.\n",
    "- **Potential for mild underestimation**: by using an independent evaluator, Double Q-Learning can occasionally underestimate values. In practice, however, slight underestimation is far less harmful than overestimation, because overestimation drives the agent toward actions it falsely believes are good, compounding errors through feedback loops in the policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
